Индекс Хирша сейчас известен всем, кто так или иначе связан с наукой и образованием. Он у всех на слуху, любой начинающий исследователь знает, что главное — заботиться о своём h-индексе, ведь чем он больше — тем лучше. Осторожные голоса тех, кто скептически смотрит на такую прямолинейную попытку оценить соотношение количества и качества научных трудов слышны уже давно — индекс Хирша активно критикуется с самого своего рождения. Но пока что предложенный в 2005-м году физиком Хорхе Хиршем способ оценивать результаты научных изысканий признается и одобряется многими представителями научного сообщества. Особенно высоко этот способ оценивается разнообразными чиновниками, призванными решать разные насущные для науки вопросы — в первую очередь, вопросы финансирования. 

Для начала — немного истории. Физик Хорхе Хирш‌ в 2005-м году предложил способ оценивать работу учёного путём расчёта индекса цитирования его статей, и [подробно описал](https://arxiv.org/abs/physics/0508025) методику этого расчёта. В самом начале той знаковой для мировой науки статьи Хирш объясняет, почему он предлагает пользоваться индексом цитирования:

«Влияние и актуальность исследований неоспоримы лишь у небольшого числа учёных, получивших Нобелевские премии. Как можно определить общее значение и актуальность результатов научного поиска отдельного учёного среди всех остальных? В мире небезграничных ресурсов такая количественная оценка (даже будучи потенциально неприятной) часто бывает необходимой для оценки и сравнения, к примеру, преподавателей университетов».‌ 

Далее он подробно описывает предлагаемый им способ оценки востребованности исследователя.

![](https://assets.discours.io/unsafe/900x/production/image/00576ff0-4e72-11ea-b394-c524ca22e7d7.png)Получение h-индекса из графика распределения статей по числу цитирований / Wikimedia Commons

Авторы статьи «Гнусные цифры» Дуглас Арнольд и Кристин Фаулер кратко определяют сущность индекса Хирша [следующим образом](https://arxiv.org/PS_cache/arxiv/pdf/1010/1010.0278v4.pdf): «h-индекс отдельного человека — это наибольшее число его (или ее) статей, каждая из которых цитировалась не менее того же числа раз». Базы научных журналов рассчитывают этот индекс у зарегистрированных там авторов автоматически.

Относительно доходчиво способ рассчитать индекс Хирша описан [здесь](https://sibac.info/blog/chto-takoe-indeks-hirsha-ili-h-indeks): 

_«Принцип расчета базируется на анализе цитирования научных работ ученого в соотношении с количеством его работ. _  


  * _Ученый опубликовал десять статей, и каждую процитировали по одному разу, h-индекс равен единице._
  * _Ученый опубликовал одну статью, и ее процитировали десять раз, его   
h-индекс равен единице._

_На основании данных идеальных примеров можно сделать важный вывод о том, что можно опубликовать одну качественную и интересную статью в известном уважаемом журнале, что будет формально равно тем же стараниям, что человек затратит на «штамповку» посредственных статей и публикацию их в слабых журналах. В реальности чаще происходят ситуации, когда у одного автора есть десять статей, но они цитируются с разной частотой, а потому подсчет индекса Хирша не всегда является простой задачей… Для получения значения h-индекса надо сделать два шага:_

  * _Выстроить статьи от большего объема их цитирования к меньшему._
  * _Определить научный труд, чей номер [не меньше числа] ссылок на него»._

**

Для полной наглядности можно представить себе сферического в вакууме учёного, который занимается генетической модификацией мышей — и время от времени пишет о своих мышках статьи, чтобы поделиться результатами опытов. 

Допустим, на данный момент у нашего учёного написано 6 статей, из них на первую статью есть 2 ссылки, на вторую — 3 (эти две статьи посвящены описанию инновационной методики и первым промежуточным результатам). Третью статью про мышек процитировали уже 5 раз (учёному удалось вывести мышей с радужно переливающейся шерстью, коллеги восхитились и стали на него активно ссылаться), на четвертую статью ученого сослались всего один раз (она была слишком сложной и к тому же вышла в предновогоднем выпуске). Пятая статья имеет на данный момент три ссылки (в ней описывается, что радужно переливающиеся мышки начали резко умнеть). И, наконец, последняя на настоящий момент статья имеет целых 70 упоминаний — в ней учёный сообщил миру, что его мышки научились печатать на планшете и написали, что проклинают своего создателя. 

Если ранжировать эти статьи от работы с максимальным числом ссылок (70) до работы с минимальным количеством упоминаний (1) и пронумеровать полученный список сверху вниз, то номер статьи, совпадающий с числом ссылок на эту статью, будет равен трём, соответственно, на момент расчёта индекс Хирша этого сферического исследователя — 3. Иными словами, у этого учёного среди всех имеющихся статей (их у него сейчас 6) есть 3 статьи, каждая из которых цитируется не менее трёх раз. То есть, если судить по h-индексу, деятельность учёного, занимающегося радужными разумными мышками, вызывает скромный, но достаточно стабильный интерес в научном сообществе.

Может показаться обидным то, что произведенный последней статьей (на которую уже успели сослаться 70 раз) фурор на размер индекса Хирша практически не повлиял — если бы её процитировали не 70 раз, а 7, индекс Хирша учёного всё равно был бы равен 3. Но нередко именно это и подаётся как главное достоинство предложенного Хорхе Хиршем способа — предполагается, что он показывает именно среднюю, стабильную востребованность работ исследователя. Опять же, если на более ранние работы учёного-мышевода продолжат ссылаться и дальше, то h-индекс у него будет расти. 

Поэтому индекс Хирша стал активно использоваться при выдаче грантов исследователям, при принятии кадровых решений в науке и так далее. Казалось бы — ура, универсальная линейка для измерения научных талантов найдена, можно раздать гранты самым достойным, расслабиться и ждать открытий и прорывов. Но реальность, как это часто случается, вносит свои поправки — к сожалению, даже самые удачные идеи при реализации на практике часто сопровождаются проблемами. В случае с индексом Хирша главная проблема заключается в том, что его можно очень легко увеличить, если написать побольше статей и негласно договориться с коллегами о цитировании. По сути, это очень похоже на взаимофолловинг — как в соцсетях можно, подписываясь на интересующие вас аккаунты, ожидать увидеть их в числе своих подписчиков, так и в науке легко организовать взаимовыгодное сотрудничество по наращиванию показателей цитирования.   


![](https://assets.discours.io/unsafe/900x/production/image/e1e3f500-4e73-11ea-b394-c524ca22e7d7.jpg)Хорхе Хирш / University of California, San Diego

Тут уместно вспомнить о научной специализации Хорхе Хирша: он, будучи физиком, по определению имеет ограничения в скорости продуцирования статей — как правило, эксперименты в физике длятся достаточно долго, а теоретические рассчёты требуют порой ещё больше времени. В своей первой статье про индекс цитируемости Хирш отметил, ‌что его исследование публикационной активности в первую очередь касалось именно физиков, но он полагает, что индекс цитируемости будет полезным и для других научных дисциплин. С ним охотно согласились множество чиновников от науки, и индекс Хирша стало модно считать вообще у всех: и у представителей естественно-научных дисциплин, и у гуманитариев, и у юристов-экономистов. 

**

Наука очень неоднородна по используемым в разных отраслях методам познания, и далеко не все исследователи имеют в своей работе естественные ограничения (например, в виде сроков экспериментов у физиков или биологов — даже эксперименты с не очень-то долго живущими мышками и мушками порой длятся годами). Есть множество научных сфер, где полёт мысли не ограничен ничем — и, соответственно, индекс Хирша можно нарастить очень и очень оперативно, не используя никаких фальсификаций. Достаточно написать несколько статей и поддерживать тесные связи с коллегами — то есть, ссылаться друг на друга почаще. 

Сколько статей было написано только лишь для того, чтобы их по предварительной договоренности процитировали для увеличения заветной цифры, от которой нередко зависят гранты, зарплаты или хотя бы доплаты? Вряд ли когда-то удастся это узнать, но некоторые тревожные симптомы можно отметить. Так, итальянские учёные Альберто Баччини, Джузеппе де Николао и Эухенио Петрович в своей статье 2019 года приводят данные, свидетельствующие о резком увеличении интенсивности научного сотрудничества внутри страны. И эта тенденция стала четко прослеживаться как раз после введения в Италии в 2011 году правил, предусматривающих, что ключевые этапы профессиональной карьеры учёного регулируются именно библиометрическими показателями. 

По [мнению](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0221212) авторов, «Наиболее вероятным объяснением специфической итальянской тенденции является общая стратегия использования цитат в итальянском научном сообществе — как в форме самоцитирования, так и в виде «клубов взаимоцитирования». Вряд ли учёные других стран менее сообразительны, чем их итальянские коллеги, и тенденции, отмеченные в развитии науки в Италии, могут оказаться типичными и для других стран. 

Если исследователя оценивают по статьям, то это совершенно естественно, что он будет активно публиковаться — чтобы, как минимум, соответствовать предъявляемым к его профессии требованиям. Может возникнуть вопрос — а почему это, собственно, плохо? Ну вот такая прагматичная мотивация, но статьи-то написаны, их много, здорово же! К сожалению, тут не всё так однозначно.﻿

> Количество научных статей в последние десятилетия неуклонно растёт — и это проблема, потому что возможности человека воспринимать информацию ограничены, а гибрид человека с компьютером, избавленный от этих ограничений, пока что создать не успели. 

Тысячи статей на тысячи разных тем — это, безусловно, прекрасно, но у любого, даже самого гениального и добросовестного учёного, в сутках всего 24 часа.

Если тысяча учёных занимается генетическими опытами с мышками, и каждый из них пишет всего лишь по одной статье в год, то желающему ознакомиться с последними достижениями в генетической модификации мышей придется в идеале хотя бы бегло просмотреть всю тысячу статей, вышедших за последний год. 

Пример, конечно, условный, и цифра намеренно завышена, но на нём хорошо видно, что многие знания — всегда в каком-то смысле многие печали. Хотя бы потому, что нельзя объять необъятное. 

Опять же, в нашем примере имеются в виду исключительно добросовестные исследователи, которые просто пишут раз в год по статье и никаких искусственных «удобрений» для роста своих наукометрических индексов не употребляют — например, не «размазывают» намеренно материал на три статьи вместо одной, чтобы коллегам приходилось цитировать их все, подкармливая таким образом авторский индекс Хирша. Но в нашем далеко не идеальном мире так, к сожалению, бывает очень редко. 

Соблазн написать как можно больше статей просто ради статей (то есть, ради показателей и отчетности) вряд ли можно измерить какими-либо индексами, но очевидно, что ему поддались очень и очень многие. Косвенный признак этого — ощутимое падение качества многих статей, а также появление большого количества так называемых хищнических, мусорных журналов, главная цель существования которых — предоставить возможность публиковать кому угодно и что угодно — на возмездной, естественно, основе.

**

Специалисты ВИНИТИ РАН‌‌ Татьяна Домнина и Оксана Хачко в статье 2015 года [говорят](http://benran.ru/SEM/Sb_15/sbornik/83.pdf) о том, что «рост количества научных журналов [в мире] в XX веке составлял не менее 3,3% ежегодно, а в начале XXI века он повысился до 4%…» И хотя в процентах вроде как на первый взгляд и немного, но с учётом того, что статьи в научных журналах надлежит читать внимательно и вдумчиво — даже такой рост оказывается значительным. Авторы связывают увеличение темпов роста числа журналов со специализацией знаний в самоорганизующейся информационной системе, и этот фактор, естественно, нельзя не учитывать. Но при этом лучше помнить и о том, что издательство научных журналов может быть чьим-то любовно развиваемым выгодным бизнесом, где спрос рождает предложение. Придуманный же Хорхе Хиршем способ оценивать «качество» исследователей по цитируемости их публикаций не мог не оказать влияния на этот спрос. 

По большому счёту, h-индекс был приговорён ещё до своего появления на свет. Чарльз Гудхарт, профессор Лондонской школы экономики и политических наук и главный советник по денежно-кредитной политике Банка Англии, ещё в 1975-м году отметил следующее: 

> «Любая наблюдаемая статистическая закономерность склонна к разрушению, как только на неё оказывается давление с целью управления [экономикой]».

Гудхард сформулировал свой закон применительно к экономическим показателям, и физик Хирш, разрабатывая свою методику оценки исследователей, вряд ли в принципе мог подумать про закон Гудхарта, всё-таки у них совсем разные сферы научных интересов. Однако деятельность «менеджеров от науки», решивших использовать индекс Хирша в качестве универсального критерия для выбора кандидатов на получение грантов или замещение должностей, привела к возникновению в науке закономерностей, аналогичных описанным Гудхартом для экономических систем. Поскольку индекс Хирша стал регулярно использоваться как инструмент принятия решений относительно финансирования, он закономерно стал объектом многочисленных манипуляций.

«Если раньше количество публикаций отражало динамику развития науки (национальной, региональной, вузовской и пр.), — [говорится](http://docplayer.ru/69484219-Naukometriya-bibliometriya.html) в недавней работе исследователей из ГПНТБ СО РАН‌, — то увязывание с этим фактором политики финансирования исследований приводит к тому, что для мониторинга научного развития придётся искать другие показатели».

Если согласиться, что индекс Хирша уже не может считаться безусловно объективным критерием качества в науке, закономерно возникает вопрос — а как тогда оценивать исследователей и их старания? Очень возможно, ответ не понравится чиновнику, потому что быстро и просто оценить результаты в такой сложной сфере, как наука, в принципе бывает невозможно. Процедура выбора нобелевских лауреатов не случайно [такая сложная и длительная](https://shkolazhizni.ru/culture/articles/91784/) — чтобы максимально объективно определить того, кто, по выражению Альфреда Нобеля, «в течение предыдущего года принёс наибольшую пользу человечеству», нужно тщательно проанализировать значительный объем информации. То есть, самым честным ответом на вопрос, как можно точно и просто посчитать эффективность исследователя, будет ответ «никак». Наиболее объективной будет только независимая экспертная оценка.

Ещё чиновникам от науки не помешало бы помнить о том, что множество открытий, изменивших жизнь человечества к лучшему, были совершены во многом благодаря случайностям. [Лизоцим](https://ru.wikipedia.org/wiki/%D0%9B%D0%B8%D0%B7%D0%BE%D1%86%D0%B8%D0%BC), например, был открыт Александром Флемингом вообще очень забавно: Флеминг, согласно распространенной версии, чихнул в чашку Петри, затем забыл о ней на пару недель, а после обнаружил, что питательная среда в забытой чашке заросла колониями бактерий — но те места, куда попала слизь от чихания (в которой присутствовал выделенный позднее Флемингом лизоцим), остались чистыми. Будущий же пенициллин вообще, можно сказать, залетел к Александру Флемингу в открытое окно — на не прикрытую крышкой чашку Петри, стоявшую на подоконнике, попали споры грибов, которые на питательной среде охотно проросли. 

![](https://assets.discours.io/unsafe/900x/production/image/cab90c10-4e70-11ea-b394-c524ca22e7d7.png)

Science Comics #1 (1946) / comicbookplus.com

Флеминг заметил, что колонии бактерий возле нитей плесени гибнут — ну, а дальше началась славная история первого в мире антибиотика, благодаря которому были спасены миллионы жизней. Кстати, научное сообщество в 1929 году [встретило](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4520913/) опубликованную Флемингом в Британском журнале экспериментальной патологии работу об открытии им пенициллина без особого энтузиазма (хорошо, что тогда ещё не додумались определять значимость исследований при помощи индексов цитирования).

**

История появления пенициллина наглядно показывает, что для гениального научного открытия часто бывает достаточно случайности. Споры плесневых грибов тысячи лет летали по свету, прорастая в разных местах и портя бактериям жизнь, но никто до Александра Флеминга этого не заметил и не оценил. 

> Для того, чтобы были научные открытия, нужны прежде всего хорошо образованные исследователи и условия для занятий наукой как таковые. 

Правда, иногда открытий приходится ждать долго, но наука, к сожалению, командными методами управляется очень плохо, зато при попытке администрировать науку подобным образом отлично развиваются имитация и фальсификации.

Для среднестатистического чиновника (как, впрочем, и для любого обывателя) нюансы исследований чаще всего совершенно непонятны, и путь к заветным открытиям может казаться неоправданно длинным. Ефрем Якушевский, один из ближайших учеников генетика Николая Ивановича Вавилова, в интервью с историком науки Эдуардом Колчинским вспоминал рассказ Вавилова о его последней встрече со Сталиным. Вавилов стремился к этой встрече, надеясь объяснить вождю советского народа сущность своих теоретических изысканий и необходимость их для решения практических задач.

![](https://assets.discours.io/unsafe/900x/production/image/f7bf0ff0-4e77-11ea-b394-c524ca22e7d7.jpg)Николай Вавилов / seeforestfortrees.com

Сталин встретил Вавилова [вопросом](http://old.ihst.ru/projects/sohist/books/os2/219-221.pdf): «Ну что, гражданин Вавилов, так и будете заниматься цветочками, лепесточками, василечками и другими ботаническими финтифлюшками? А кто будет заниматься повышением урожайности сельскохозяйственных культур?» — и прочитанную ему Вавиловым мини-лекцию выслушал безо всякого интереса. Зато, судя по всему, «народный академик» Лысенко говорил «большому учёному» Сталину вещи намного более понятные и приятные, нежели чем Вавилов, вскорости брошенный по сфабрикованному обвинению в тюрьму и умерший там через несколько лет. Трофим Лысенко мушками-дрозофилами начальство не раздражал, потому что считал генетику лженаукой, долгих экспериментов не проводил и вообще не мелочился — он щедро раздавал обещания накормить СССР в кратчайшие сроки. Правда, используемые Лысенко методы были либо уже давно и хорошо известны (например, посадка картофеля верхушками клубней вместо целых) либо на практике проваливались — например, массовая [яровизация](https://ru.wikipedia.org/wiki/%D0%AF%D1%80%D0%BE%D0%B2%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F) (контролируемое охлаждение семян перед их посевом) не дала практически никакой прибавки урожая — и не удивительно, потому что Лысенко часто сразу внедрял нововведения, минуя эксперименты и статистическую обработку данных. Но свои неудачи народный академик быстро маскировал обещаниями новых «прорывов».

Поразительно, но эти трагические страницы отечественной науки до сих пор вызывают споры — у точки зрения, что учёный должен обязательно и срочно приносить пользу, а не копаться в мушках, мышках или цветочках, и сейчас находится немало сторонников. Кстати, можно не сомневаться — если бы в СССР в 1930–40-е годы изобрели некий аналог индекса Хирша‌, то у Трофима Лысенко точно не возникло бы проблем с показателями цитируемости. Если же наукометрические показатели оказались бы слишком высокими у ставших неугодными Вавилова и его коллег, то эту проблему решили бы очень просто и быстро — их статьи банально изъяли бы из библиотек и объявили запрет на их упоминание.

Считается, что самая первая попытка оценить научную деятельность посредством анализа публикаций была предпринята еще в XIX веке — Чарльз Бэббидж [предложил](https://dspace.spbu.ru/bitstream/11701/16369/1/Moskaleva_Akoev.pdf) использовать количество научных публикаций для оценки известности ученого, но эта идея была встречена скептически. В начале XX века появился прообраз импакт-фактора — формального показателя значимости научного журнала, и с 1960-х годов он ежегодно [рассчитывается](https://ru.wikipedia.org/wiki/%D0%98%D0%BC%D0%BF%D0%B0%D0%BA%D1%82-%D1%84%D0%B0%D0%BA%D1%82%D0%BE%D1%80) американским Институтом научной информации. 

Импакт-фактор показывает, сколько раз в среднем каждая опубликованная в данном конкретном журнале статья цитируется в течение двух последующих лет после выхода этого журнала. То есть, импакт-фактор используется для измерения общего, усредненного качества статей в журнале. 

Расчет индивидуального показателя востребованности учёного — это относительно новое явление. По сути, Хирш первым предложил простой и, казалось бы, эффективный способ оценивать вклад рядовых исследователей в науку (предложенный на год позже [g-индекс](https://ru.wikipedia.org/wiki/G-%D0%B8%D0%BD%D0%B4%D0%B5%D0%BA%D1%81), индекс Эгга, также основанный на анализе цитирования статей, широкого применения и признания не получил). Российский РИНЦ — национальная библиографическая база данных научного цитирования, в которой собраны публикации российских авторов, а также информация о цитировании этих публикаций — ровесник индекса Хирша, и проблемы у этого индекса научного цитирования те же самые, что и у h-индекса — [им тоже очень легко манипулировать](https://trv-science.ru/2014/09/23/rints-ot-primitivnogo-moshennichestva-do-rastleniya-maloletnikh/). H-индекс практически сразу же перестал быть объективным методом оценки, поскольку его увеличение превратилось в самоцель. Да, в идеальном мире индекс Хирша и подобные ему методы расчёта были бы очень хорошими рабочими инструментами для приблизительной оценки эффективности работы учёных, но в полном несовершенств мире реальном любая наукометрия неизбежно приводит к имитациям и перекосам. 

Возможно, исследователям XXI века намного целесообразнее не включаться в гонку наращивания индексов цитирования, но сообща требовать не привязывать финансирование науки исключительно и только к формальным показателям. А менеджерам науки стоило бы смириться с тем, что выделяемые на науку средства не превращаются в «прорывные» результаты в строго определенный срок. 

В конце концов, достаточно наличия определенной материальной базы, учёных, любящих свою работу, и строгого следования научной этике — и научные открытия обязательно будут. Имитация же научной деятельности опасна не только тем, что она вытесняет настоящую науку, оттягивая на себя ресурсы. Имитация и фальсификации в науке подрывают доверие к науке как к институту, и последствия этого [могут быть](https://discours.io/articles/social/smert-ekspertizy-chem-obestsenivanie-znaniya-ugrozhaet-chelovechestvu) фатальными. 
